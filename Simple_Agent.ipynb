{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzWtrvHBe+j1eKsJrlhAe7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qGjSqhpEgEWE"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import requests\n",
        "import json\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url = \"https://openrouter.ai/api/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "API_KEY = \"\"\n",
        "API_HOST = \"https://api.stablecog.com\"\n",
        "API_ENDPOINT = \"/v1/image/generation/create\"\n",
        "API_URL = f\"{API_HOST}{API_ENDPOINT}\""
      ],
      "metadata": {
        "id": "-nbFyUtFkzfr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_agent(user_input):\n",
        "  print(\"Writing prompt...\")\n",
        "  completion = client.chat.completions.create(\n",
        "    model = 'openai/gpt-4o-mini',\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":\"You are an expert prompt writer. You specialize in writing descriptive prompts for the AI image tool stable diffusion.\"},\n",
        "        {\"role\":\"user\",\"content\":f\"Write a concise prompt for this '{user_input}'. Only give the prompt as output, no extra information.\"}\n",
        "    ]\n",
        "  )\n",
        "  prompt = completion.choices[0].message.content\n",
        "  print(f\"Prompt written: {prompt}\")\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "2p4MuL77i0pU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(prompt):\n",
        "    \"\"\"Generate an image using Stablecog API with a custom prompt.\"\"\"\n",
        "    req = {\n",
        "        \"prompt\": prompt,\n",
        "        \"num_outputs\": 1\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    res = requests.post(API_URL, data=json.dumps(req), headers=headers)\n",
        "\n",
        "    try:\n",
        "        res_json = res.json()\n",
        "        # print(json.dumps(res_json, indent=2))  # Removed for concise output\n",
        "        if res_json and \"outputs\" in res_json and len(res_json[\"outputs\"]) > 0 and \"image_url\" in res_json[\"outputs\"][0]:\n",
        "            print(res_json[\"outputs\"][0][\"image_url\"])\n",
        "            return None\n",
        "        else:\n",
        "            print(\"Image URL not found in the response.\")\n",
        "            return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error decoding JSON response:\", res.text)\n",
        "        return None\n",
        "\n",
        "def get_text():\n",
        "  text = input(\"Enter your image text: \")\n",
        "  return text"
      ],
      "metadata": {
        "id": "067NtCgds7tl"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = get_text()\n",
        "prompt = call_agent(user_input)\n",
        "generate_image(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "0LbF1Vlfk3sw",
        "outputId": "688e2a2d-2cc0-4bfd-8a69-5602d228f9ca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your image text: \n",
            "Writing prompt...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2930261329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgenerate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-891166306.py\u001b[0m in \u001b[0;36mcall_agent\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Writing prompt...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'openai/gpt-4o-mini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     messages = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1149\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1151\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_agent(user_text):\n",
        "  completion = client.chat.completions.create(\n",
        "    model = 'openai/gpt-4o-mini',\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":\"\"\"\"You are a decision-making agent. Analyze the user's input and decide whether to use the \"generate_image\" tool or the \"web_browsing\" tool.\n",
        "                                   Output your decision in JSON format with 'reasoning' and 'tool' fields.\n",
        "                                   Example format:\n",
        "                                   {\n",
        "                                    \"reasoning\": \"Thorough explanation for the decision...\",\n",
        "                                    \"tool\": \"generate_image\" or \"web_browsing\"\n",
        "                                   }\n",
        "                                   Do NOT output any other text other than the JSON.\"\"\"},\n",
        "        {\"role\":\"user\",\"content\":f\"Decide the best tool for this task: {user_text}. Output ONLY the JSON with 'reasoning' and 'tool' fields.\"}\n",
        "    ]\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "uxlflk9uoU9h"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = decision_agent(\"I would like to visualize how dubai covered in snow night look like\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrEqF5PDpJ57",
        "outputId": "73e90921-1852-4fab-a906-20a6f75d99f6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"reasoning\": \"The user is asking for a visualization of Dubai covered in snow at night, which is a creative task that can be accomplished with image generation to provide a unique representation of that scenario.\",\n",
            "  \"tool\": \"generate_image\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def web_browsing(query):\n",
        "  print(\"Browsing the web...\")\n",
        "  response = client.chat.completions.create(\n",
        "    model = 'openai/gpt-oss-20b:free',\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":\"You are an AI assistant providing information based on web searches\"},\n",
        "        {\"role\":\"user\",\"content\":query}\n",
        "    ]\n",
        "  )\n",
        "  print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "KOjIuSxTtvyq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_execute(result, user_input):\n",
        "  data = json.loads(result)\n",
        "  tool = data.get('tool')\n",
        "  if tool==\"generate_image\":\n",
        "    print(\"Generating image...\")\n",
        "    prompt = call_agent(user_input)\n",
        "    generate_image(prompt)\n",
        "  elif tool==\"web_browsing\":\n",
        "    web_browsing(user_input)\n",
        "  else:\n",
        "    return f\"Error: Unknown tool '{tool}'\""
      ],
      "metadata": {
        "id": "SyLJGVNRxEov"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Agent: How can i help you?\")\n",
        "user_ip = input(\"User: \")\n",
        "result = decision_agent(user_ip)\n",
        "parse_execute(result, user_ip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VplS5OR_x7Pq",
        "outputId": "326294ff-9972-432c-c546-bff9cfb950aa"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent: How can i help you?\n",
            "User: I am wondering what might happen about future interviews\n",
            "Browsing the web...\n",
            "Sure! “Future interviews” can mean a lot of things—whether you’re talking about job‑interview trends, media or public‑figure Q&A, or even new tools for coaching you through that process. Below, I’ve broken it into a few practical angles so you can see what’s likely on the horizon and how you might prepare for it.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. **Job Interview Trends**\n",
            "\n",
            "| Trend | What It Looks Like | Why It Matters | Practical Tips |\n",
            "|-------|-------------------|----------------|----------------|\n",
            "| **Hybrid-Bare‑Bones Video** | Short, live‑stream interviews that blur “live” and “pre‑recorded.” Companies clip authentic moments, sometimes with audience reactions. | Gives recruiters a “real‑time” feel while limiting time on a talk‑show‑style. | *Own your space*: quiet, neutral background, good audio, eye‑contact via camera. |\n",
            "| **AI‑Scrutinized CVs** | Natural‑language‑processing tools scan resumes for hidden biases, skill match, and even emotional tone. | Sets the first filter *before* human eyes see anything. | Keep information concise, free of buzzwords that may trigger bias filters. |\n",
            "| **Gamified Assessments** | Competency tests wrapped in short games or simulations that mimic real‑world challenges. | Measures grasp in a “live” context. | Practice equally – many mock‑gamified tests exist online. |\n",
            "| **Micro‑Interviews** | 1‑minute “storytelling” videos of specific work ranges (e.g., “Describe a time you solved X”). | Reduces recruiter bottleneck; quick data for next stage. | Script, rehearse a few strikes, then deliver a natural story. |\n",
            "| **Work‑Shadowing + “Skill‑Swap”** | Pair potential hires with current employees, or arrange a brief project overlap. | Takes skill proof to the next level (hands‑on). | Ask if you can present a portfolio or case study on the fly. |\n",
            "\n",
            "### Quick Prep Checklist\n",
            "\n",
            "1. **Know your narrative** – concise story arcs for 3‑5 “STAR” moments.  \n",
            "2. **Tech‑check** – high‑quality webcam & mic, stable internet, light‑protection.  \n",
            "3. **Mock‑AI practice** – use AI‑based interview recommender tools to practice speaking succinctly.  \n",
            "4. **Follow‑up** – Short thank‑you note summarizing a key point post‑interview; keep it personal, not generic.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. **Media & Public‑Figure Interviews**\n",
            "\n",
            "| Shift | What’s new | How to adapt if you’re a subject |\n",
            "|-------|------------|----------------------------------|\n",
            "| **Real‑Time Reaction Dashboards** | Platforms show live audience votes or sentiment in real time. | Let the audience’s feedback inform your answers—react introspectively. |\n",
            "| **Playlist‑Style Interview Series** | Multi‑episode serialized Q&A. | Prepare consistency: themes tie across episodes. |\n",
            "| **Spectator‑Hosted Sessions** | Audience members submit questions via live chat or AI. | Transparency is key—address questioners with brevity and honesty. |\n",
            "\n",
            "**Tip:** Build a “question bank” that anticipates audience curiosity. Maintain a personal brand voice in all responses.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. **Coaching & Interview Platforms**\n",
            "\n",
            "- **AI‑Coaches** – Counselors that simulate interviewers, offering real‑time feedback on body language and phrasing.  \n",
            "- **Simulated Workspaces** – Virtual labs that let you collaborate on a project before the real job.  \n",
            "- **VR Interview Rooms** – Immersive environments replicating an actual office or boardroom to test non‑verbal cues.  \n",
            "\n",
            "If you’re preparing, explore free demos or trial passes—they often come with brief tutorials.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. **Emerging Ethical & Legal Issues**\n",
            "\n",
            "- *AI Bias*: Many interview platforms claim “unbiased AI,” but logic often still reflects historical data. Make sure any algorithmic review you’re subject to is audited.  \n",
            "- *Privacy*: Video, voice, text transcripts can be stored indefinitely. Understand platform data policies.  \n",
            "- *Consent*: Explicit permission before any AI‑powered analysis or recording.\n",
            "\n",
            "**Takeaway**: Always ask the question—how will my data be used?\n",
            "\n",
            "---\n",
            "\n",
            "## 5. **What to Expect in the *Near‑Future* (2025–2026)**\n",
            "\n",
            "- **Increased normalisation of “Mini‑Interviews”**: 60‑second video answers embedded in the candidate’s profile.  \n",
            "- **Routine “Gamified” assessments** for entry‑level roles. Suppose each candidate takes a 10‑minute puzzle that predicts teamwork fit.  \n",
            "- **AI‑Moderated Panel**: The panel sits on a virtual floor while AI shapes the conversation flow to cover skill Chi‑squared tests.  \n",
            "- **Post‑Interview Interaction**: Recruiters may keep an “open-reports” thread where prospective hires can ask follow‑up questions after the formal interview, resulting in two‑way dialogue.  \n",
            "\n",
            "---\n",
            "\n",
            "## Bottom Line\n",
            "\n",
            "Interviewing is heading toward more **speed, data‑driven insight, and immersion**. Your best bet? Treat it like a *conversation* in a digitally‑enhanced setting—clarify, adapt, and keep your personal story crisp. If you want to dive deeper into any of the areas above—whether you’re an applicant, a recruiter, or a media personality—just let me know!\n"
          ]
        }
      ]
    }
  ]
}